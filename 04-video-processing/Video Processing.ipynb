{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72db3765",
   "metadata": {},
   "source": [
    "# Introduction to Video Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d5d22",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "- Create a sequence of video frames by applying signal processing techniques learned in the course.\n",
    "- Solving problems of video streaming optimization by applying different tools and filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7eadb1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "So far, we have dealt with the idea of image processing, where images can be thought of as 2D signals and we learnt how to manipulate these signals using different filters.  Now, as you know, a video is essentially a sequence of images displayed at a certain frame rate. This makes image processing the cornerstone of video processing. Thus, initially in this lab, we explore how image processing techniques can be extended to videos.\n",
    "\n",
    "In this lab, we will delve deep into the realm of video signals. A key concept we'll explore is how videos containing predominantly low-frequency components – where the difference between consecutive frames is minimal – can be effectively compressed without substantial loss in quality. \n",
    "\n",
    "We'll utilize the Fourier Transform, a powerful mathematical tool, to analyze the frequency content of video signals. This invaluable information will help inform your decisions on video compression strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9154ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage import io, color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b395d",
   "metadata": {},
   "source": [
    "### Basics of Video Processing\n",
    "The first part of our journey into video processing begins with extraction of frames from a video. Videos, at their core, are a sequence of images, or frames, that are displayed at a rapid rate. When we watch a video, our brains interpret this rapid sequence of slightly differing images as motion. The process of extracting frames from a video is simply the act of capturing these individual images at each step in the sequence. \n",
    "\n",
    "When we're done processing, we'll need to convert these frames back into a video format. This is essentially the reverse of the extraction process, where we stitch the sequence of images back together into a continuous stream. Let us write functions for both the extraction of the frames and the reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "501714e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path):\n",
    "    \"\"\"\n",
    "    Extract frames from a video file and return them as a list of images.\n",
    "\n",
    "    This function uses OpenCV's VideoCapture class to open the video file at \n",
    "    the provided path. It then enters a while loop that continues for as long \n",
    "    as the video file is open.\n",
    "\n",
    "    Within the loop, it reads each frame in the video one at a time. If a frame \n",
    "    is successfully read (indicated by `ret` being True), it is added to the \n",
    "    list of frames.\n",
    "\n",
    "    If a frame cannot be read (which usually means that the end of the video has \n",
    "    been reached), `ret` will be False, and the loop will exit.\n",
    "\n",
    "    Finally, the VideoCapture object is released to free up system resources, \n",
    "    and the list of frames is returned.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        frames (list of ndarray): A list where each element is a numpy array \n",
    "            representing an image.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad4d88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_video(frames, output_video_file, fps=30):\n",
    "    '''\n",
    "    Convert a list of frames into a video\n",
    "\n",
    "    Parameters:\n",
    "    frames (list): List of frames. Each frame should be a numpy array\n",
    "    output_video_file (str): Output video file name\n",
    "    fps (int): Frames per second of the output video\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    # Get size (width, height) from the first image\n",
    "    height, width, layers = frames[0].shape\n",
    "    size = (width, height)\n",
    "\n",
    "    # Initialize the video writer\n",
    "    out = cv2.VideoWriter(output_video_file, cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
    "\n",
    "    # Write frames to the video writer\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release the video writer\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c18485",
   "metadata": {},
   "source": [
    "Next, we'll delve into an important concept in video processing - frame difference. **Frame difference** is a technique that allows us to quantify the changes between two consecutive frames in a video. Essentially, it helps us detect motion in the video by highlighting the parts of the frame that have changed. In a static image, the difference between it and the next image (if they are identical) would be zero. But in a video, especially in high action segments, the frame difference could reveal interesting patterns.\n",
    "\n",
    "Let's try to write a function that calculates the frame difference. We already have a function that extracts all frames from a video, so we can utilize that. What we need to do next is to iterate over these frames, and for each frame, calculate the difference with the next one. There are multiple ways to calculate this difference, but a simple way is to subtract the pixel values of one frame from the corresponding pixel values of the next frame. This will give us a new image that represents the difference between the two frames.\n",
    "\n",
    "Here's a skeleton for the function. Your task will be to complete the function using what you've learned so far. You can take help of the `absdiff`function that is available in the cv2. You can refer to the [documentation here](https://docs.opencv.org/3.4/d2/de8/group__core__array.html#ga6fef31bc8c4071cbc114a758a2b79c14) (link opens in new tab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6f50a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frame_differences(frames):\n",
    "    \"\"\"\n",
    "    Calculate the difference between consecutive frames in a list.\n",
    "\n",
    "    This function takes a list of frames and computes the absolute difference \n",
    "    between each pair of consecutive frames, creating a new \"difference frame\" \n",
    "    that highlights the regions of the frame that have changed.\n",
    "\n",
    "    Args:\n",
    "        frames (list of ndarray): A list of numpy arrays where each array \n",
    "            represents a frame in the video.\n",
    "\n",
    "    Returns:\n",
    "        frame_diffs (list of ndarray): A list of numpy arrays where each array \n",
    "            represents the absolute difference between two consecutive frames.\n",
    "    \"\"\"\n",
    "    frame_diffs = []\n",
    "    for i in range(...):\n",
    "        diff = ...\n",
    "        frame_diffs.append(diff)\n",
    "    return frame_diffs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e17de6",
   "metadata": {},
   "source": [
    "### Coming back to Image Processing\n",
    "Before doing anything significant in video processing, we would need to revisit some tools that you have worked with in Image Processing. Load the video `goal.mp4` and extract its frames and frame differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a60b42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = ...\n",
    "frame_diffs = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9108ead4",
   "metadata": {},
   "source": [
    "## Part 1: Frequency Analysis\n",
    "Frequency analysis plays a crucial role in image and video processing, offering a different perspective from the spatial domain representation, which we're more intuitively familiar with.\n",
    "\n",
    "In the frequency domain, an image is represented by the frequencies of the signals that make up the image. High frequencies correspond to rapid changes in pixel values, such as edges or fine details, while low frequencies correspond to slow changes, such as smooth gradients or larger homogeneous areas.\n",
    "\n",
    "**Question 1.1:** Write a function that converts an image to the frequency domain. Keep in mind, we will write this functions with respect to videos and not images. Run the code below to see if you wrote the right code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14aa6b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_frequency(frame_diffs):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes the frequency components of the first difference frame.\n",
    "\n",
    "    This function takes as input a list of difference frames (typically derived from a sequence \n",
    "    of video frames). It then selects the first difference frame, converts it to grayscale, and \n",
    "    computes its 2D Fourier Transform. The Fourier Transform is then shifted to center the \n",
    "    zero-frequency components. The magnitude spectrum, which represents the distribution of \n",
    "    frequencies in the image, is computed and both the original grayscale difference frame and \n",
    "    its magnitude spectrum are displayed side by side.\n",
    "\n",
    "    Parameters:\n",
    "    - frame_diffs (list of numpy.ndarray): A list of difference frames, where each frame is \n",
    "      an image represented as a 2D or 3D array.\n",
    "\n",
    "    Note:\n",
    "    - This function uses OpenCV for image processing and matplotlib for visualization.\n",
    "    - Only the first frame in `frame_diffs` is processed and visualized in this function.\n",
    "    - The magnitude spectrum is displayed in logarithmic scale for better visibility of details.\n",
    "\n",
    "    \"\"\"\n",
    "    # Select appropriate frame\n",
    "    example_diff = ...\n",
    "    gray_diff = cv2.cvtColor(example_diff, cv2.COLOR_BGR2GRAY)\n",
    "    # Compute Fourier Transform and then perform a shift on it\n",
    "    f = np.fft.fft2(...)\n",
    "    fshift = ...\n",
    "    # Compute associated magnitude of shifted frequency in dB \n",
    "    magnitude_spectrum = ...\n",
    "\n",
    "    plt.subplot(1,2,1), plt.imshow(gray_diff, cmap = 'gray')\n",
    "    plt.title('Input Image'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(1,2,2), plt.imshow(magnitude_spectrum, cmap = 'gray')\n",
    "    plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_frequency(...) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c9696",
   "metadata": {},
   "source": [
    "After analyzing the frequency of the frame differences, you might start noticing patterns and correlations between the frequency spectrum and the pace of the video. In a fast-paced video, such as an action game, changes between frames are generally more dramatic, and in contrast, in a slower-paced video, like a walking simulator game, changes between frames are usually subtler, so we anticipate a concentration of lower frequencies.\n",
    "\n",
    "**Question 1.2:** We have two games, Game A and Game B. Game A is a fast-paced game with rapid transitions and high-speed movements. Game B is a slow-paced game with slow transitions and minimal movements. Out of the two images below, which image resembles Game A and Game B respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b50eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = io.imread('img1.png')\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Image 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c504fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = io.imread('img2.png')\n",
    "plt.imshow(img2)\n",
    "plt.title(\"Image 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6f4dd",
   "metadata": {},
   "source": [
    "### Miscellaneous: Switching Phases\n",
    "\n",
    "For fun, let us try to switch the phases between the fast-paced game and slow-paced game, and see the results. This would involve us to extract the frames and writing up a function to switch the phases between two frames and applying it throughout the video. You may need to use the following function that converts polar to cartesian coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62310fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol2cart(rho, phi):\n",
    "    x = rho * np.cos(phi)\n",
    "    y = rho * np.sin(phi)\n",
    "    z = x + y*1j\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af252ae3",
   "metadata": {},
   "source": [
    "**Question 1.3:** Write a function `swap_phase` that takes as argument two complex numbers $z_1$ and $z_2$ and returns complex numbers $z_3$ and $z_4$, such that \n",
    "\n",
    "$$\n",
    "|z_3| = |z_1|,~~\\rm{arg}(z_3) = \\rm{arg}(z_2),\n",
    "$$\n",
    "and \n",
    "$$\n",
    "|z_4| = |z_2|,~~\\rm{arg}(z_4) = \\rm{arg}(z_1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6ef066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_phase(z1, z2):\n",
    "    \"\"\"\n",
    "    Swaps the phase angles of two complex numbers while preserving their magnitudes.\n",
    "\n",
    "    Parameters:\n",
    "    - z1 (complex): The first complex number.\n",
    "    - z2 (complex): The second complex number.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing two complex numbers:\n",
    "        - The first complex number has the magnitude of z1 and the phase angle of z2.\n",
    "        - The second complex number has the magnitude of z2 and the phase angle of z1.\n",
    "    \"\"\"\n",
    "    z3 = ...\n",
    "    z4 = ...\n",
    "    return (z3, z4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d37b83",
   "metadata": {},
   "source": [
    "**Question 1.4:** Now, let us extend this function to images (or frames) where we swap the phase between two images by taking the fourier transfrom and applying the `swap_phase` function. Let us further apply this new function to the first frame from both the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8557f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_phase_img(img1,img2):\n",
    "    \"\"\"\n",
    "    Swaps the phase spectrum of the Fourier Transforms of two images.\n",
    "\n",
    "    This function performs a Fourier transform on both input images, swaps the phase\n",
    "    spectra while keeping the magnitude spectra unchanged, and then inversely transforms\n",
    "    back to the spatial domain.\n",
    "\n",
    "    Parameters:\n",
    "    - img1 (numpy.ndarray): The first 2D image array.\n",
    "    - img2 (numpy.ndarray): The second 2D image array.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing two 2D image arrays:\n",
    "        - The first image has the magnitude spectrum of img1 and the phase spectrum of img2.\n",
    "        - The second image has the magnitude spectrum of img2 and the phase spectrum of img1.\n",
    "\n",
    "    Note:\n",
    "    - The output images will be in real domain, with any imaginary components discarded.\n",
    "    \"\"\"    \n",
    "    f1 = ...\n",
    "    f2 = ...\n",
    "    swapped1, swapped2 = ...\n",
    "    img1_swapped = ...\n",
    "    img2_swapped = ...\n",
    "\n",
    "    return img1_swapped, img2_swapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e2c1e",
   "metadata": {},
   "source": [
    "Complete the code below which takes the first frame from two video clips `game_A.mp4` and `game_B.mp4` and swaps the phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870df681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the frames from game_A.mp4 and game_B.mp4 and select the first frame\n",
    "video_path1 = 'game_A.mp4' \n",
    "frames1 = ...\n",
    "\n",
    "video_path2 = 'game_B.mp4' \n",
    "frames2 = ...\n",
    "\n",
    "example_frame1 = ...\n",
    "example_frame2 = ... \n",
    "\n",
    "#Swap the phases\n",
    "phase_switched_frame1, phase_switched_frame2 = swap_phase(...)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(phase_switched_frame1.real, cmap='gray')\n",
    "plt.title('Phase Switched Frame 1')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(phase_switched_frame2.real, cmap='gray')\n",
    "plt.title('Phase Switched Frame 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a9817",
   "metadata": {},
   "source": [
    "If you do not see a lot going on in your output, that should be fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e9f5ef",
   "metadata": {},
   "source": [
    "**Question 1.5:** Let us now apply the swapped phase to the whole video and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4395fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_switched_frames_1 = []\n",
    "phase_switched_frames_2 = []\n",
    "\n",
    "#Apply the swap_phase function and append it onto the frame arrays\n",
    "#The number of iterations is dependent on the video containing the lesser number of frames (Why?)\n",
    "for i in range(min(len(frames1), len(frames2))): \n",
    "    frame1, frame2 = ...\n",
    "    #Write instructions to append new frames onto the two empty arrays\n",
    "    ...\n",
    "    ...\n",
    "\n",
    "phase_switched_frames_1 = [np.real(frame) for frame in phase_switched_frames_1]\n",
    "phase_switched_frames_2 = [np.real(frame) for frame in phase_switched_frames_2]\n",
    "\n",
    "#Apply the frames to video functions for the two new arrays\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c73558",
   "metadata": {},
   "source": [
    "## Part 2: Filters\n",
    "Digital filters are a key concept in signal processing and will play a significant role in our lab. We've learned in our coursework how filters can modify signals in useful ways. In the context of images and video, which are just two-dimensional (or three-dimensional if we consider the color channels) signals, filters can help us enhance features, remove noise, and extract useful information from the raw signal. There are many types of digital filters, but we'll focus primarily on **Low-Pass Filter**, **High-Pass Filter**, **Edge Detection Filter**, **Image Cropping Filter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59c7e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread('fgp.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a9f27f",
   "metadata": {},
   "source": [
    "**Question 2.1:**  We have \"black-boxed\" the low-pass filter and the edge-detection filter. Can you implement the remaining functions? Futhermore, apply all the filters onto the image loaded below. (**Hint:** The Image-Cropping filter can simply be implemented in a single line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae2cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowpass_filter(image, cutoff_frequency_ratio=0.01, high_pass=False, order=1):\n",
    "    \"\"\"Apply a Lowpass Butterworth filter to a colour image. \n",
    "    \n",
    "    This function should wrap the skimage.filters.butterworth function \n",
    "    so that it is applied to each channel of the image individually, and\n",
    "    then the reconstructed image returned.\n",
    "    \n",
    "    Args:\n",
    "        image (array): Array representing a multi-channel image.\n",
    "        cutoff_frequency_ratio (float): Indicates the position of the frequency\n",
    "            cutoff as a percentage of the length of the spectrum.\n",
    "        high_pass (bool): If True, will act as a highpass filter. Otherwise, it\n",
    "            will be a lowpass filter.\n",
    "        order (float): The order of the filter; how quickly the frequency \n",
    "            response drops to 0.\n",
    "            \n",
    "    Returns:\n",
    "        array: A multi-channel image that has had the Butterworth filter\n",
    "        applied to each channel indepedently.\n",
    "    \"\"\"\n",
    "    # Set up a NumPy array of the same size as the image\n",
    "    filtered_image = np.zeros_like(image)\n",
    "\n",
    "    # Loop over the channels; apply the filter one channel at a time\n",
    "    # and store the results in the filtered_image\n",
    "    for idx in range(filtered_image.shape[2]):\n",
    "        filtered_image[:,:,idx] = skimage.filters.butterworth(\n",
    "            image[:,:,idx], \n",
    "            cutoff_frequency_ratio=cutoff_frequency_ratio, \n",
    "            high_pass=high_pass, \n",
    "            order=order\n",
    "        )\n",
    "        \n",
    "    return filtered_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpf_img = ...\n",
    "plt.imshow(lpf_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66ee8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_detector(image):\n",
    "    \"\"\"Apply an edge detection filter to a colour image. \n",
    "    \n",
    "    This function should wrap the skimage.filters.butterworth function \n",
    "    so that it is applied to each channel of the image individually, and\n",
    "    then the reconstructed image returned.\n",
    "    \n",
    "    Add keyword arguments to this function as needed.\n",
    "    \n",
    "    Args:\n",
    "        image (array): Array representing a multi-channel image.\n",
    "            \n",
    "    Returns:\n",
    "        array: A multi-channel image that has had the Butterworth filter\n",
    "        applied to each channel indepedently.\n",
    "    \"\"\"\n",
    "    filtered_image = np.zeros_like(image, dtype=float)\n",
    "    \n",
    "    for idx in range(filtered_image.shape[2]):\n",
    "        filtered_image[:,:,idx] = skimage.filters.sobel(image[:,:,idx])\n",
    "        \n",
    "    return filtered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d3b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_detected_image = ...\n",
    "io.imshow(edge_detected_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "512c395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highpass_filter(image, cutoff_frequency_ratio=0.35, high_pass=True, order=1):\n",
    "    \"\"\"Apply a High-Pass Butterworth filter to a colour image. \n",
    "    \n",
    "    This function should wrap the skimage.filters.butterworth function \n",
    "    so that it is applied to each channel of the image individually, and\n",
    "    then the reconstructed image returned.\n",
    "    \n",
    "    Args:\n",
    "        image (array): Array representing a multi-channel image.\n",
    "        cutoff_frequency_ratio (float): Indicates the position of the frequency\n",
    "            cutoff as a percentage of the length of the spectrum.\n",
    "        high_pass (bool): If True, will act as a highpass filter. Otherwise, it\n",
    "            will be a lowpass filter.\n",
    "        order (float): The order of the filter; how quickly the frequency \n",
    "            response drops to 0.\n",
    "            \n",
    "    Returns:\n",
    "        array: A multi-channel image that has had the Butterworth filter\n",
    "        applied to each channel indepedently.\n",
    "    \"\"\"\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpf_img = ...\n",
    "plt.imshow(hpf_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7059d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img, x, y, width, height):\n",
    "    \"\"\"\n",
    "    This function crops an image to the desired size.\n",
    "    \n",
    "    Args:\n",
    "    img (np.array): Input image.\n",
    "    x (int): x-coordinate of the top left corner of the desired crop.\n",
    "    y (int): y-coordinate of the top left corner of the desired crop.\n",
    "    width (int): Desired width of the crop.\n",
    "    height (int): Desired height of the crop.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Cropped image.\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_image = ...\n",
    "io.imshow(cropped_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30796465",
   "metadata": {},
   "source": [
    "## Part 3: Video Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96714c66",
   "metadata": {},
   "source": [
    "Given a short video clip from a game of FIFA, you are required to experiment with different parameters of a low-pass Butterworth filter to reduce the high-frequency content (noise) in the video. You should aim for the highest level of compression (reduced data) without significantly compromising the quality of the video. As part of this exercise, you should also inspect the video frame-by-frame and examine the frequency spectrum of individual frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73133271",
   "metadata": {},
   "source": [
    "### Step 1: Extract Frames from Video\n",
    "\n",
    "The first step will be to extract the frames from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5667c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'goal.mp4' \n",
    "frames = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e073c",
   "metadata": {},
   "source": [
    "### Step 2: Analyze a Single Frame\n",
    "\n",
    "Before applying the filter to the entire video, it's a good idea to start with a single frame. This will allow you to get a sense of how the filter affects the image and what parameter values might be appropriate. You can display the frame before and after filtering and also look at the frequency spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_frame = ... # take the first frame as an example\n",
    "filtered_frame = ... # Apply the low-pass filter \n",
    "\n",
    "# Display the original and filtered frames for comparison\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1), plt.imshow(cv2.cvtColor(example_frame, cv2.COLOR_BGR2RGB)), plt.title('Original')\n",
    "plt.subplot(1,2,2), plt.imshow(cv2.cvtColor(filtered_frame, cv2.COLOR_BGR2RGB)), plt.title('Filtered')\n",
    "plt.show()\n",
    "\n",
    "# Analyze the frequency spectrum of the original and filtered frames\n",
    "analyze_frequency([example_frame])\n",
    "analyze_frequency([filtered_frame])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cec900",
   "metadata": {},
   "source": [
    "### Step 3: Apply Filter to Entire Video\n",
    "\n",
    "Once you're satisfied with the result on the example frame, you can apply the filter to the entire video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "378a6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_frames = []\n",
    "... # Write your code to apply the filter to all the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41422e1c",
   "metadata": {},
   "source": [
    "### Step 4: Create Compressed Video\n",
    "\n",
    "Now that you have the filtered frames, you can convert them back into a video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f82b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc2939",
   "metadata": {},
   "source": [
    "## Extra: Using Different Filters\n",
    "After doing the last exercise, can you think of a way to apply all the other filtes you made to the same video? Write your code below. For the sake of simplicity, add the following line to your code before appending your frames when dealing with edge detector filter:\n",
    "\n",
    "`normalized_edge_frame = cv2.normalize(edge_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5232c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
